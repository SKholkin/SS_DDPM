{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d432cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7738589",
   "metadata": {},
   "source": [
    "## Model. Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bf64b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes, grayscale):\n",
    "        self.inplanes = 64\n",
    "        if grayscale:\n",
    "            in_dim = 1\n",
    "        else:\n",
    "            in_dim = 3\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_dim, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, (2. / n)**.5)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # because MNIST is already 1x1 here:\n",
    "        # disable avg pooling\n",
    "        #x = self.avgpool(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(x)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas\n",
    "    \n",
    "    def get_representation(self, x):\n",
    "#         self.eval()\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "#         x = self.layer4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def resnet18(num_classes):\n",
    "    \"\"\"Constructs a ResNet-18 model.\"\"\"\n",
    "    model = ResNet(block=BasicBlock, \n",
    "                   layers=[2, 2, 2, 2],\n",
    "                   num_classes=num_classes,\n",
    "                   grayscale=True)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc98f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "train_dataset = MNIST(root='MNIST', download=True, train=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "val_dataset = MNIST(root='MNIST', download=True, train=False, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b40c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100\n",
    "    \n",
    "def train(model, optimizer, train_loader,  n_epochs=10):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            ### FORWARD AND BACK PROP\n",
    "            logits, probas = model(features)\n",
    "            cost = F.cross_entropy(logits, targets)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            cost.backward()\n",
    "\n",
    "            ### UPDATE MODEL PARAMETERS\n",
    "            optimizer.step()\n",
    "\n",
    "            ### LOGGING\n",
    "            if not batch_idx % 50:\n",
    "                print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
    "                       %(epoch+1, n_epochs, batch_idx, \n",
    "                         len(train_loader), cost))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.set_grad_enabled(False): # save memory during inference\n",
    "            print('Epoch: %03d/%03d | Train: %.3f%%' % (\n",
    "                  epoch+1, n_epochs, \n",
    "                  compute_accuracy(model, train_loader, device=device)))\n",
    "\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d3a20bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/002 | Batch 0000/0938 | Cost: 2.4674\n",
      "Epoch: 001/002 | Batch 0050/0938 | Cost: 0.2613\n",
      "Epoch: 001/002 | Batch 0100/0938 | Cost: 0.2263\n",
      "Epoch: 001/002 | Batch 0150/0938 | Cost: 0.1247\n",
      "Epoch: 001/002 | Batch 0200/0938 | Cost: 0.1589\n",
      "Epoch: 001/002 | Batch 0250/0938 | Cost: 0.0675\n",
      "Epoch: 001/002 | Batch 0300/0938 | Cost: 0.0403\n",
      "Epoch: 001/002 | Batch 0350/0938 | Cost: 0.1786\n",
      "Epoch: 001/002 | Batch 0400/0938 | Cost: 0.1176\n",
      "Epoch: 001/002 | Batch 0450/0938 | Cost: 0.0367\n",
      "Epoch: 001/002 | Batch 0500/0938 | Cost: 0.0800\n",
      "Epoch: 001/002 | Batch 0550/0938 | Cost: 0.0328\n",
      "Epoch: 001/002 | Batch 0600/0938 | Cost: 0.1290\n",
      "Epoch: 001/002 | Batch 0650/0938 | Cost: 0.0583\n",
      "Epoch: 001/002 | Batch 0700/0938 | Cost: 0.1797\n",
      "Epoch: 001/002 | Batch 0750/0938 | Cost: 0.1221\n",
      "Epoch: 001/002 | Batch 0800/0938 | Cost: 0.0414\n",
      "Epoch: 001/002 | Batch 0850/0938 | Cost: 0.0158\n",
      "Epoch: 001/002 | Batch 0900/0938 | Cost: 0.1115\n",
      "Epoch: 001/002 | Train: 98.133%\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 002/002 | Batch 0000/0938 | Cost: 0.0250\n",
      "Epoch: 002/002 | Batch 0050/0938 | Cost: 0.0240\n",
      "Epoch: 002/002 | Batch 0100/0938 | Cost: 0.0116\n",
      "Epoch: 002/002 | Batch 0150/0938 | Cost: 0.0349\n",
      "Epoch: 002/002 | Batch 0200/0938 | Cost: 0.0391\n",
      "Epoch: 002/002 | Batch 0250/0938 | Cost: 0.0463\n",
      "Epoch: 002/002 | Batch 0300/0938 | Cost: 0.1276\n",
      "Epoch: 002/002 | Batch 0350/0938 | Cost: 0.1101\n",
      "Epoch: 002/002 | Batch 0400/0938 | Cost: 0.1590\n",
      "Epoch: 002/002 | Batch 0450/0938 | Cost: 0.0488\n",
      "Epoch: 002/002 | Batch 0500/0938 | Cost: 0.0612\n",
      "Epoch: 002/002 | Batch 0550/0938 | Cost: 0.0599\n",
      "Epoch: 002/002 | Batch 0600/0938 | Cost: 0.0096\n",
      "Epoch: 002/002 | Batch 0650/0938 | Cost: 0.1268\n",
      "Epoch: 002/002 | Batch 0700/0938 | Cost: 0.0038\n",
      "Epoch: 002/002 | Batch 0750/0938 | Cost: 0.0106\n",
      "Epoch: 002/002 | Batch 0800/0938 | Cost: 0.0310\n",
      "Epoch: 002/002 | Batch 0850/0938 | Cost: 0.0869\n",
      "Epoch: 002/002 | Batch 0900/0938 | Cost: 0.1379\n",
      "Epoch: 002/002 | Train: 98.785%\n",
      "Time elapsed: 0.58 min\n",
      "Total Training Time: 0.58 min\n",
      "Test accuracy: 98.71%\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model = resnet18(10)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train(model, optimizer, train_dataloader, n_epochs=2)\n",
    "\n",
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    print('Test accuracy: %.2f%%' % (compute_accuracy(model, val_dataloader, device=device)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32612b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'resnet18_fid_mnist.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0999de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from models import UNet\n",
    "device = 'cuda'\n",
    "model = UNet(1, 32, (1, 2, 4), time_emb_dim=16)\n",
    "T = 1000\n",
    "from functools import partial\n",
    "from schedule import noising_sch\n",
    "import torch.nn as nn\n",
    "\n",
    "noising_sch = partial(noising_sch, T=T)\n",
    "\n",
    "from beta import KL, sufficient_stats, alpha_beta, get_dist\n",
    "\n",
    "class BetaUnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = UNet(1, 32, (1, 2, 4), time_emb_dim=16)\n",
    "\n",
    "    def forward(self, suff_stats, t):\n",
    "        return F.sigmoid(self.model(suff_stats, t))\n",
    "    \n",
    "\n",
    "    def generate_alpha_normed(model, filename='beta_ssddpm_samples.png'):\n",
    "        model.eval()\n",
    "        x_t = torch.rand([batch_size, 1, 28, 28]).to(device)\n",
    "        suff_stats = sufficient_stats(x_t, torch.tensor([T], device=device).repeat(batch_size))\n",
    "        alphas = noising_sch(torch.tensor([T], device=device).repeat(batch_size))\n",
    "\n",
    "        suff_stats_normed = suff_stats / alphas.reshape([-1, 1, 1, 1]).repeat([1, 1, 28, 28])\n",
    "\n",
    "        to_plot = []\n",
    "\n",
    "        samples_history = []\n",
    "\n",
    "        for t in range(T, 1, -1):\n",
    "            t_batch = torch.tensor([t], device=device).repeat(batch_size)\n",
    "            x_0 = model(suff_stats_normed, t_batch)\n",
    "\n",
    "            mu = noising_sch(t_batch)\n",
    "            alphas += mu\n",
    "\n",
    "            dist = get_dist(mu, x_0)\n",
    "            x_t = dist.sample()\n",
    "            samples_history.append(x_t)\n",
    "\n",
    "            alphas += mu\n",
    "\n",
    "            suff_stats += sufficient_stats(x_t, torch.tensor([t - 1], device=device).repeat(batch_size))\n",
    "\n",
    "            suff_stats_normed = suff_stats / alphas.reshape([-1, 1, 1, 1]).repeat([1, 1, 28, 28])\n",
    "            to_plot.append(suff_stats_normed[:4, :, 10, 10].squeeze())\n",
    "\n",
    "        t_batch = torch.tensor([1], device=device).repeat(batch_size)\n",
    "        x_0 = model(suff_stats_normed, t_batch)\n",
    "\n",
    "        return x_0\n",
    "\n",
    "gen_model = BetaUnet().to(device)\n",
    "gen_model.load_state_dict(torch.load('beta_ddpm_the_best.pth'))\n",
    "\n",
    "\n",
    "samples = gen_model.generate_alpha_normed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59e3c8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 938/938 [00:06<00:00, 152.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 39/39 [06:04<00:00,  9.36s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(71.3666-0.1368j)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "def compute_fid_mnist(dataloader, gen_model, n_fake_samples=10000):\n",
    "    device = 'cuda'\n",
    "    model = resnet18(10)\n",
    "    model.load_state_dict(torch.load('resnet18_fid_mnist.pth'))\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    true_dist = []\n",
    "        \n",
    "    for batch, _ in tqdm(iter(dataloader)):\n",
    "        \n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            representation = model.get_representation(batch).cpu()\n",
    "            true_dist.append(representation)\n",
    "        \n",
    "    true_dist = torch.cat(true_dist, dim=0)\n",
    "    mu_true = torch.mean(true_dist, dim=0)\n",
    "    cov_true = torch.cov(true_dist.T)\n",
    "    \n",
    "    batch_size = 256\n",
    "    gen_model.eval()\n",
    "    \n",
    "    fake_dist = []\n",
    "    \n",
    "    for i in tqdm(range(n_fake_samples // batch_size)):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch = gen_model.generate_alpha_normed()\n",
    "            representation = model.get_representation(batch).cpu()\n",
    "            fake_dist.append(representation)\n",
    "    \n",
    "    fake_dist = torch.cat(fake_dist, dim=0)\n",
    "    \n",
    "    mu_fake = torch.mean(fake_dist, dim=0)\n",
    "    cov_fake = torch.cov(fake_dist.T)\n",
    "    \n",
    "    conv_sqrtm = 2 * sqrtm(cov_true @ cov_fake)\n",
    "    \n",
    "    fid = torch.norm(mu_true - mu_fake) ** 2 + torch.trace(cov_true + cov_fake - conv_sqrtm)\n",
    "    \n",
    "    return fid\n",
    "\n",
    "compute_fid_mnist(train_dataloader, gen_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdedf17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
